2.1、Dolphindb样本生成脚本

2.1.1、device_readings.csv数据集分区脚本getJobStat()

login("admin","123456")
//导入csv文件为内存表
fp_info = '/media/xllu/aa/device/devices_info.csv'
fp_readings = '/media/xllu/aa/device/devices_readings.csv'

dp_readings = "dfs://db_range_dfs"
//dp_readings = "/home/xllu/database/db_range"
// 创建两张表的 schema
colnames_info 	= `device_id`api_version`manufacturer`model`os_name
colnames_readings = `time`device_id`battery_level`battery_status`battery_temperature`bssid`cpu_avg_1min`cpu_avg_5min`cpu_avg_15min`mem_free`mem_used`rssi`ssid

types_info = `SYMBOL`INT`STRING`STRING`STRING
types_readings = `DATETIME`SYMBOL`INT`STRING`DOUBLE`STRING`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`SYMBOL

schema_info 	= table(colnames_info, types_info)
schema_readings = table(colnames_readings, types_readings)

time_range=2016.11.15T00:00:00 + 86400 * 0..4
id_range=('demo'+lpad((0..10*300)$STRING,6,"0"))$SYMBOL

time_schema=database('',RANGE,time_range)
id_schema=database('',RANGE,id_range)
// 创建分区数据库并定义分区方式
if(existsDatabase(dp_readings)){
	dropDatabase(dp_readings)
}
db = database(dp_readings , COMPO, [time_schema,id_schema])

// 从 CSV 导入 readings 表的数据到 readings 数据库并完成数据分区操作
timer loadTextEx(db, `readings_pt, `time`device_id, fp_readings, , schema_readings)
//38s
// 从 CSV 导入 device_info 表的数据到 device_info 内存表
device_info = loadText(fp_info, , schema_info)

// 加载 readings 表（不包括表连接）
readings = loadTable(db, `readings_pt)
2.1.2、TAQ20070807.csv-TAQ20070810.csv数据集分区脚本
login("admin","123456")
FP_TAQ = '/media/xllu/aa/TAQ/original/'device_
FP_SAMPLE_TB = FP_TAQ + 'TAQ20070807.csv'
orig_tb_schema = extractTextSchema(FP_SAMPLE_TB)
// 查看 orig_tb_schema
// 将列名调整为小写避免与 DolphinDB 内置的 SYMBOL, DATE, TIME 等保留关键字产生冲突
cols = lower(orig_tb_schema.name)
schema = table(cols, orig_tb_schema.type)
timer sample_tb = loadText(FP_SAMPLE_TB, , schema) 
sample_freq_tb = select count(*) from sample_tb group by symbol
mmid_tb = select count(*) from sample_tb group by mmid
// 8369 rows, [symbol, count], 分到 100 个 buckets
BIN_NUM = 100
buckets = cutPoints(sample_freq_tb.symbol, BIN_NUM, sample_freq_tb.count)
// [A, ABL, ACU, ..., ZZZ], 101 个边界
buckets[BIN_NUM] = `ZZZZZZ		// 调整最右边界
DATE_RANGE = 2007.08.05..2007.08.12
// 创建数据库分区方案
date_schema   = database('', VALUE, DATE_RANGE)
symbol_schema = database('', RANGE, buckets)
//FP_DB = FP_TAQ + 'db/'
//db = database(FP_DB, COMPO, [date_schema, symbol_schema])
db_path="dfs://db_compound_dfs"
if(existsDatabase(db_path)){
	dropDatabase(db_path)
}
db = database(db_path, COMPO, [date_schema, symbol_schema])
fps = FP_TAQ + (exec filename from files(FP_TAQ) order by filename)
// 导入到分布式数据库中
timer {getJobStat()

	for (fp in fps) {
		job_id_tmp = fp.strReplace(".csv", "")
		job_id_tmp1=split(job_id_tmp,"/")
		job_id=job_id_tmp1[6]
		job_name = job_id
		submitJob(job_id, job_name, loadTextEx{db, `taq, `date`symbol, fp})
		print now() + ": 已导入 " + fp
	}
	getRecentJobs(size(fps))
}

2.2、Mongodb样本生成脚本

2.2.1、readings数据集分区脚本getJobStat()
use config
db.chunks.find()
db.settings.save({"_id":"chunksize","value":1024}) 
use admin
sh.enableSharding("device_pt")
Use device_pt
db.device_readings.createIndex({time:1,device_id:1}) 
//为节约导入时间，我们先导入数据，在mongoimport文件目录下输入以下脚本
./mongoimport -h localhost:40000 -d db_nopt -c device_readings --file /media/xllu/aa/device/device_readings_mongo.csv --type csv --columnsHaveTypes --fields "time.date(2006-01-02T15:04:05),device_id.string(),battery_level.int32(),battery_status.string(),battery_temperature.double(),bssid.string(),cpu_avg_1min.double(),cpu_avg_5min.double(),cpu_avg_15min.double(),mem_free.double(),mem_used.double(),rssi.double(),ssid.string()" --parseGrace skipRow --numInsertionWorkers 12
//回到shell界面，输入以下脚本完成分区
sh.shardCollection("device_pt.device_readings",{"time":1,"device_id":1})

2.2.2、TAQ数据集分区脚本

use config
db.chunks.find()
db.settings.save({"_id":"chunksize","value":1024}) 
use admin
sh.enableSharding("taq_pt_db")
Use device_pt
db.device_readings.createIndex({date:1,symbol:1}) 
/*为节约导入时间，我们将数据分为16个小数据表进行分别导入，在shell中输入以下bash脚本*/
#!/bin/bash
for f in /media/xllu/aa/TAQ/mongo_split/*.csv ; do
        /usr/bin/mongoimport \
        -h localhost \
        --port 40000 \
        -d taq_pt_db \
        -c taq_pt_col \
        --type csv \
        --columnsHaveTypes \
 --fields "symbol.string(),date.date(20060102),time.date(15:04:05),bid.double(),ofr.double(),bidsiz.int32(),ofrsiz.int32(),mode.int32(),ex.string(),mmid.string()" \
        --parseGrace skipRow \
        --numInsertionWorkers 12 \
        --file $f
    echo "文件 $f 导入完成"这个
done
//回到shell界面，输入以下脚本完成分区
sh.shardCollection("device_pt.device_readings",{"time":1,"device_id":1})
